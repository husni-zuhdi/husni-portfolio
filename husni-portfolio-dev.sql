PRAGMA foreign_keys=OFF;
BEGIN TRANSACTION;
CREATE TABLE IF NOT EXISTS blogs (
            id TEXT PRIMARY KEY NOT NULL,
            name TEXT NOT NULL,
            source TEXT NOT NULL,
            filename TEXT NOT NULL,
            body TEXT NOT NULL
        );
INSERT INTO blogs VALUES('001-g','How to setup simple kubernetes cluster with gce','Github','https://api.github.com/repos/husni-zuhdi/husni-blog-resources/contents/001-how-to-setup-simple-kubernetes-cluster-with-gce/README.md',replace('<h1>How to setup simple Kubernetes Cluster with GCE</h1>\n<h2>Chapter 1: Introduction</h2>\n<p>As an Site Reliability Engineer in the modern software company, I can’t never keep my hand from tinkering and operating Kubernetes clusters. If you’re a junior like me, you’ll mostly doing operation task with Kubernetes like creating new deployment, make sure configmap variable values are correct, or investigating why this CRD is not working. I rarely have an opportunity to deploy a kubernetes cluster from beginning. So in this article I want to show you how I deploy my own staging kubernetes cluster.</p>\n<p>We’ll start with the tools needed to spin up a minimal Kubernetes cluster. I will deploy a Kubernetes cluster in Google Cloud Platform using Compute Engine Instances. More or less we need these resources in Google Compute Engine:</p>\n<ol>\n<li>A VPC with at least one subnet.</li>\n<li>A Public IP address for control plane node.</li>\n<li>One Router and one NAT in the same region.</li>\n<li>Several security groups.</li>\n<li>An Instance template.</li>\n<li>One Compute Engine as control plane node.</li>\n<li>Two Compute Engine as regular nodes.</li>\n</ol>\n<h2>Chapter 2: Prepare Infrastructure Resources</h2>\n<p>To setup the infrastructure resources, we’ll use <code>terrafrom</code> and <code>terragrunt</code> masked as <code>task</code> subcommand as our Infrastructure as Code tool. You can see the code repository in <a href="https://github.com/husni-zuhdi/husni-blog-resources/tree/main/001-g-how-to-setup-simple-kubernetes-cluster-with-gce/infrastructure">here</a>.</p>\n<p>Before applying infra resources in this folder, please read <a href="https://github.com/husni-zuhdi/husni-blog-resources/tree/main/001-g-how-to-setup-simple-kubernetes-cluster-with-gce/../000-main-infrastructure/README.md">000-main-infrastructure</a>.</p>\n<p>Task subcommand you''ll need to execute:</p>\n<pre><code class="language-bash"># Plan or Dry run all terraform manifest\ntask plan-all -- 001-how-to-setup-simple-kubernetes-cluster-with-gce/infrastructure\n# Apply all terraform manifest. Will create all infra resources\ntask apply-all -- 001-how-to-setup-simple-kubernetes-cluster-with-gce/infrastructure\n# Destroy all infra resources. Will delete all resource. Use with cautions!\ntask destroy-all -- 001-how-to-setup-simple-kubernetes-cluster-with-gce/infrastructure\n</code></pre>\n<h2>Chapter 3: Bootstrap Kubernetes Cluster</h2>\n<p>Let’s start with your control plane or master node.</p>\n<ol>\n<li>\n<p>Check if the MAC address is available with this command</p>\n<pre><code class="language-bash">ip link\nifconfig -a\n</code></pre>\n</li>\n<li>\n<p>Check product ID with this command</p>\n<pre><code class="language-bash">sudo cat /sys/class/dmi/id/product_uuid\n</code></pre>\n</li>\n<li>\n<p>(Optional) Check available ports with command <code>nc</code>. If no <code>nc</code> is installed, please install it with the command <code>sudo apt install netcut</code>.</p>\n</li>\n<li>\n<p>Install Container Runtime. For <strong>container.d</strong> on D<strong>ebian</strong>, use this command [1]</p>\n<pre><code class="language-bash"># Add Docker''s official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg gpg -y\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# Add the repository to Apt sources:\necho \\n  &quot;deb [arch=&quot;$(dpkg --print-architecture)&quot; signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \\n  &quot;$(. /etc/os-release &amp;&amp; echo &quot;$VERSION_CODENAME&quot;)&quot; stable&quot; | \\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\nsudo apt-get install containerd.io -y\n</code></pre>\n</li>\n<li>\n<p>Install <code>kubeadm, kubectl, and kubelet</code>. For Debian</p>\n<pre><code class="language-bash"># Get kubernetes stable v1.28\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n\n# This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list\necho ''deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /'' | sudo tee /etc/apt/sources.list.d/kubernetes.list\n\nsudo apt-get update\nsudo apt-get install -y kubelet kubeadm kubectl\nsudo apt-mark hold kubelet kubeadm kubectl\n</code></pre>\n</li>\n<li>\n<p>Set IpTables [2]</p>\n<pre><code class="language-bash">cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf\noverlay\nbr_netfilter\nEOF\n\nsudo modprobe overlay\nsudo modprobe br_netfilter\n\n# sysctl params required by setup, params persist across reboots\ncat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.ipv4.ip_forward                 = 1\nEOF\n\n# Apply sysctl params without reboot\nsudo sysctl --system\n</code></pre>\n</li>\n<li>\n<p>Verify the net_filter and overlay. The result will be like this</p>\n<p><img src="https://raw.githubusercontent.com/husni-zuhdi/husni-blog-resources/main/001-g-how-to-setup-simple-kubernetes-cluster-with-gce/../attachments/001_terminal.png" alt="Terminal Result" /></p>\n<pre><code class="language-bash">lsmod | grep br_netfilter\nlsmod | grep overlay\n</code></pre>\n</li>\n<li>\n<p>Verify the system variables</p>\n<pre><code class="language-bash">sudo sysctl \\nnet.bridge.bridge-nf-call-iptables \\nnet.bridge.bridge-nf-call-ip6tables \\nnet.ipv4.ip_forward\n</code></pre>\n</li>\n<li>\n<p>Check the cgroup that is used in the node</p>\n<pre><code class="language-bash">ps -p 1\n# Result. This node use systemd\n# PID TTY          TIME CMD\n# 1 ?        00:00:01 systemd\n</code></pre>\n</li>\n<li>\n<p>Configure cgroup for systems driver and restart containerd [3]</p>\n<pre><code class="language-bash">cat &lt;&lt;EOT | sudo tee /etc/containerd/config.toml\n[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc]\n  [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options]\n    SystemdCgroup = true\nEOT\nsudo systemctl restart containerd\n</code></pre>\n</li>\n<li>\n<p>Do steps 1 - 10 for all nodes.</p>\n</li>\n<li>\n<p>Bootstrap Cluster in the master node. Save the join command in the last line of stdout [4]</p>\n<pre><code class="language-bash">export INT_IP_ADDR=&quot;CONTROL_PLANE_INTERNAL_IP&quot;\nexport EXT_IP_ADDR=&quot;CONTROL_PLANE_EXTERNAL_IP&quot;\nexport POD_CIDR=&quot;10.244.0.0/16&quot;\n\nsudo kubeadm init \\n--apiserver-cert-extra-sans=$EXT_IP_ADDR \\n--apiserver-advertise-address $INT_IP_ADDR \\n--pod-network-cidr=$POD_CIDR\n</code></pre>\n</li>\n<li>\n<p>Setup kubeconfig after cluster bootstrap finished</p>\n<pre><code class="language-bash">mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre>\n</li>\n<li>\n<p>Install Cilium or Weave-Net (Deprecated) as CNI from master node</p>\n<ol>\n<li>\n<p>Install Weave-Net (Deprecated) [5]</p>\n<pre><code class="language-bash">kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml\n</code></pre>\n<ol>\n<li>\n<p>Update the IP_ALLOC environment variable in weavenet daemon set the same as pod network cidr [6]</p>\n<pre><code class="language-bash">containers:\n  - name: weave\n    env:\n    - name: IPALLOC_RANGE\n      value: 10.244.0.0/16\n</code></pre>\n</li>\n<li>\n<p>Join the other node with our master node. The command is in step 12 stdout</p>\n<pre><code class="language-bash">sudo kubeadm join INT_IP_ADDR:6443 --token TOKEN \\n        --discovery-token-ca-cert-hash CA_CERT_HASH\n</code></pre>\n</li>\n</ol>\n</li>\n<li>\n<p>Install Cilium with helm</p>\n<ol>\n<li>\n<p>Make sure linux kernel version is <code>&gt;= 4.9.17</code></p>\n<pre><code class="language-bash">uname -r\n</code></pre>\n</li>\n<li>\n<p>Taint master node [7]</p>\n<pre><code class="language-bash">kubectl taint nodes k8s-master-001 node.cilium.io/agent-not-ready=true:NoExecute\n</code></pre>\n</li>\n<li>\n<p>Install Helm</p>\n<pre><code class="language-bash">curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre>\n</li>\n<li>\n<p>Add Cilium repository and Install Cilium CRD [8]</p>\n<pre><code class="language-bash">helm repo add cilium https://helm.cilium.io/\nhelm install cilium cilium/cilium --version 1.15.1 --namespace kube-system\n</code></pre>\n</li>\n<li>\n<p>Install Cilium CLI</p>\n<pre><code class="language-bash">CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)\nCLI_ARCH=amd64\nif [ &quot;$(uname -m)&quot; = &quot;aarch64&quot; ]; then CLI_ARCH=arm64; fi\ncurl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}\nsha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum\nsudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin\nrm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}\n</code></pre>\n</li>\n<li>\n<p>Join the other node with our master node. The command is in step 12 stdout</p>\n<pre><code class="language-bash">sudo kubeadm join INT_IP_ADDR:6443 --token TOKEN \\n        --discovery-token-ca-cert-hash CA_CERT_HASH\n</code></pre>\n</li>\n<li>\n<p>(Optional) Restart unmanaged pods</p>\n<pre><code class="language-bash">kubectl get pods --all-namespaces -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,HOSTNETWORK:.spec.hostNetwork --no-headers=true | grep ''&lt;none&gt;'' | awk ''{print &quot;-n &quot;$1&quot; &quot;$2}'' | xargs -L 1 -r kubectl delete pod\n</code></pre>\n</li>\n<li>\n<p>Start verification process</p>\n<pre><code class="language-bash">cilium status\n</code></pre>\n</li>\n</ol>\n</li>\n</ol>\n</li>\n<li>\n<p>Your kubernetes cluster is ready 🥳</p>\n</li>\n</ol>\n<h2>Chapter 4: Access the Kubernetes Cluster</h2>\n<p>After you kubernetes cluster is ready, you can get your kubeconfig file in <code>~/.kube/config</code> file.\nCopy that config file and you can access your kubernetes cluster from you local device.\nDon''t forget to change the internal control plane IP to the external one.</p>\n<pre><code class="language-bash">--  server: https://CONTROL_PLANE_INTERNAL_IP:6443\n++  server: https://CONTROL_PLANE_EXTERNAL_IP:6443\n\n</code></pre>\n<h2>References</h2>\n<p>[1] <a href="https://docs.docker.com/engine/install/debian/#install-using-the-repository">Install using the apt repository | Docker Docs</a></p>\n<p>[2] <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#forwarding-ipv4-and-letting-iptables-see-bridged-traffic">Forwarding IPv4 and letting iptables see bridged traffic</a></p>\n<p>[3] <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#forwarding-ipv4-and-letting-iptables-see-bridged-traffic">Configuring the systemd cgroup driver</a></p>\n<p>[4] <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#initializing-your-control-plane-node">Initializing your control-plane node</a></p>\n<p>[5] <a href="https://www.weave.works/docs/net/latest/kubernetes/kube-addon/">Weave Kubernetes Addon</a></p>\n<p>[6] <a href="https://www.weave.works/docs/net/latest/install/installing-weave">Installing Weave</a></p>\n<p>[7] <a href="https://docs.cilium.io/en/stable/installation/taints/#taint-effects">Taint Effects</a></p>\n<p>[8] <a href="https://docs.cilium.io/en/stable/installation/k8s-install-helm/">Installation Using Helm</a></p>\n','\n',char(10)));
INSERT INTO blogs VALUES('002-g','Devops dojo cicd with github action','Github','https://api.github.com/repos/husni-zuhdi/husni-blog-resources/contents/002-devops-dojo-cicd-with-github-action/README.md',replace('<h1>DevOps Dojo: Explore CI/CD with Github Action</h1>\n<h2>Chapter 1: Introduction</h2>\n<p>WIP</p>\n<h2>Chapter 2: Create your first Github Action</h2>\n<p>WIP</p>\n<h2>Chapter 3: Now real thing, let''s create our pipeline</h2>\n<p>WIP</p>\n<h2>References</h2>\n','\n',char(10)));
INSERT INTO blogs VALUES('003-g','Migrate my portfolio website from actix web to axum','Github','https://api.github.com/repos/husni-zuhdi/husni-blog-resources/contents/003-migrate-my-portfolio-website-from-actix-web-to-axum/README.md',replace('<h1>Migrate my portfolio website from actix-web to axum</h1>\n<h2>🤔 Why?</h2>\n<p>I started to take care of my portfolio project again after abandoning it for 2 months. My main reason is that I want to make something that has real usage (even though I don''t have real traffic right away). One of the reasons is that I want to write codes.</p>\n<p><img src="https://raw.githubusercontent.com/husni-zuhdi/husni-blog-resources/main/003-g-migrate-my-portfolio-website-from-actix-web-to-axum/../attachments/003_husni_portfolio_commit_over_time.png" alt="My portfolio website activity" /></p>\n<p>During the brainstorming phase, I want to have a feature to pull data from my github repository. I have a <a href="https://github.com/husni-zuhdi/husni-blog-resources">husni-blog-resources</a> repository consisting of my writing about technical stuff (mostly related to Infrastructure as a Code and Site Reliability Engineering). In this website, I have a <code>Blogs</code> page to show my blog. I don''t want to waste my time copy-paste my articles. So, why not create a feature to pull all my writings from that repo and render them on my portfolio website? That seems doable.</p>\n<p>I found <a href="https://github.com/XAMPPRocky/octocrab/tree/main">Octocrab</a> Github API Client for Rust that was created by <a href="https://github.com/XAMPPRocky">XAMPPRocky</a> (Thanks for creating such an amazing project 🙏). Then I noticed during the build process, this library used <code>hyper</code> and <code>tower</code> libraries. Both libraries are under the <a href="https://tokio.rs">tokio</a> ecosystem, an async runtime for Rust.</p>\n<p>In the previous version, I used the <code>actix-web</code> web framework since it was easy to learn and quite straightforward. I tried to learn <code>tokio</code> before and tbh, as a self-learner programmer, it was a steep learning curve to tackle and I don''t have any reason to learn that hard if there is a more easy option to choose. However, after seeing the Octocrab library, I want to implement it on my portfolio website. If I use <code>actix</code> and <code>tokio</code> at the same time it seems (and turns out to become) inefficient. Why do I need two async runtime? Nobody needs that many async runtime.</p>\n<p>So in this release of the <a href="https://github.com/husni-zuhdi/husni-portfolio">husni-porfolio</a> website, I want to use <code>octocrab</code> and move to <a href="https://github.com/tokio-rs/axum">axum</a>, one of the lightweight web frameworks powered by <code>tokio</code>.</p>\n<h2>✍ Process</h2>\n<p>The migration process is quite simple as my portfolio website is quite simple since <code>actix</code> and <code>axum</code> use similar concepts. The first thing is to install <code>axum</code> by adding these lines to the <code>Cargo.toml</code></p>\n<pre><code class="language-toml">axum = &quot;0.7.5&quot;\ntokio = { version = &quot;1.0&quot;, features = [&quot;full&quot;] }\ntower = { version = &quot;0.4&quot;, features = [&quot;util&quot;] }\ntower-http = { version = &quot;0.5.0&quot;, features = [&quot;fs&quot;, &quot;trace&quot;] }\n</code></pre>\n<p>Under the <code>cmd</code> folder, I need to change the async runtime in the <code>main.rs</code> file from <code>actix</code> to <code>axum</code>. The first time I tried to build my application after migrating all of the source codes, I forgot to update this async runtime and keep me puzzled about <em>why does actix still compiled after I removed it from the dependencies list?</em>. Sometimes you just forget the most obvious thing.</p>\n<pre><code class="language-rust">// actix\n#[actix_web::main]\nasync fn main() -&gt; std::io::Result&lt;()&gt; {\n...\n}\n\n// axum\n#[tokio::main]\nasync fn main() -&gt; std::io::Result&lt;()&gt; {\n...\n}\n</code></pre>\n<p>Then in the <code>internal</code> folder, I need to change my handler and routing functions to suit the <code>axum</code> specification. The structure of my <code>internal</code> library can be simplified into something like this:</p>\n<ul>\n<li><code>router.rs</code> is where all of my router functions are located. They render HTML with <a href="https://github.com/djc/askama">askama</a> and do additional processing.</li>\n<li><code>handler.rs</code> process config, load blogs data, setup state data, configure routing, and running HTTP server.</li>\n</ul>\n<p>Changes in <code>router.rs</code> are quite simple as we only need to change function parameter types and return types while the inner mechanism of each function stays relatively the same. For example, in the below snippet of the <code>get_blog</code> function, I change the types of <code>path</code> and <code>blogs_data</code> parameters also with the return type. Other than that I only added pattern matching to handle errors and edge cases.</p>\n<pre><code class="language-rust">// actix\npub async fn get_blog(\n    path: web::Path&lt;String&gt;,\n    blogs_data: web::Data&lt;BlogsData&gt;,\n) -&gt; Result&lt;impl Responder&gt; {\n    let blog_id = path.into_inner();\n    let blog_data = blogs_data\n        .blogs\n        .iter()\n        .filter(|blog| blog.id == blog_id)\n        .next()\n        .expect(&quot;Failed to get blog name with id {blog_id}&quot;);\n\n    let blog = Blog {\n        id: &amp;blog_id,\n        name: &amp;blog_data.name,\n        filename: &amp;blog_data.filename,\n        body: &amp;blog_data.body,\n    }\n    .render()\n    .expect(&quot;Failed to render blog.html&quot;);\n    Ok(Html(blog))\n}\n\n// axum\npub async fn get_blog(Path(path): Path&lt;String&gt;, State(app_state): State&lt;AppState&gt;) -&gt; Html&lt;String&gt; {\n    let state = app_state\n        .blogs_data\n        .blogs\n        .iter()\n        .filter(|blog| &amp;blog.id == &amp;path)\n        .next();\n    debug!(&quot;BlogData: {:?}&quot;, &amp;state);\n\n    match state {\n        Some(_) =&gt; {}\n        None =&gt; {\n            warn!(\n                &quot;Failed to get blog with ID {}. Retunre 404 Not Found&quot;,\n                &amp;path\n            );\n            return get_404_not_found().await;\n        }\n    }\n\n    let blog_data = state.unwrap();\n    let blog = Blog {\n        id: path.clone().as_str(),\n        name: &amp;blog_data.name,\n        filename: &amp;blog_data.filename,\n        body: &amp;blog_data.body,\n    }\n    .render();\n\n    match blog {\n        Ok(res) =&gt; {\n            info!(&quot;Blog ID {} askama template rendered.&quot;, &amp;path);\n            Html(res)\n        }\n        Err(err) =&gt; {\n            error!(&quot;Failed to render blog.html. {}&quot;, err);\n            get_500_internal_server_error()\n        }\n    }\n}\n</code></pre>\n<p>For the <code>handler.rs</code> function, most of the changes are on how to serve the HTTP server. In <code>actix</code> you can use <code>actix_web::HttpServer</code> to spin up the new HTTP server while in <code>axum</code> the equal step is to use <code>axum::Router</code>. To configure the service routing, I prefer the way <code>axum</code> handles it because it looks a lot cleaner than <code>actix_web</code>. I also add a fallback function to default any not-routed endpoint to the <code>get_404_not_found</code> function.</p>\n<p>Two things that I see as big differences are how <code>axum</code> handles state and static file(s). In <code>actix</code> I can serve multiple states to be shared between routes but in <code>axum</code> you can only share one state type (but you can create <a href="https://docs.rs/axum/latest/axum/extract/struct.State.html#substates">substates</a>). Tbh, it''s not a big problem for me (at least for now) since I only do read operations from the state.</p>\n<p>To handle static files (like CSS, images, and icons) I used to create a dedicated function that utilizes <code>actix_files</code>. But in <code>axum</code> I tried to do the same thing but was unable to find an easy way. In the end, I read the documentation of <code>tower_http</code> that adds an additional feature to serve a static file in the <code>tokio</code> ecosystem. I used <code>tower_http::ServerFile</code> and <code>tower_http::ServerDir</code> as a nested service to serve all of my static files.</p>\n<pre><code class="language-rust">// actix\nHttpServer::new(move || {\n	App::new()\n		.wrap(middleware::Logger::default())\n		.app_data(web::Data::new(blogs_data.clone()))\n		.app_data(web::Data::new(config.clone()))\n		.service(web::resource(&quot;/&quot;).route(web::get().to(profile)))\n		.service(web::resource(&quot;/statics/{static_file}&quot;).route(web::get().to(statics)))\n		.service(web::resource(&quot;/blogs&quot;).route(web::get().to(get_blogs)))\n		.service(web::resource(&quot;/blogs/{blogid}&quot;).route(web::get().to(get_blog)))\n		.service(web::resource(&quot;/version&quot;).route(web::get().to(get_version)))\n		.service(web::resource(&quot;/not-found&quot;).route(web::get().to(get_404_not_found)))\n})\n.bind((endpoint, port))\n.expect(&quot;Failed to start Http Server&quot;)\n.run()\n.await\n\n// axum\nlet app = Router::new()\n	.route(&quot;/&quot;, get(get_profile))\n	.route(&quot;/not-found&quot;, get(get_404_not_found))\n	.route(&quot;/version&quot;, get(get_version))\n	.route(&quot;/blogs&quot;, get(get_blogs))\n	.route(&quot;/blogs/:blog_id&quot;, get(get_blog))\n	.nest_service(&quot;/statics&quot;, get_service(ServeDir::new(&quot;./statics/favicon/&quot;)))\n	.nest_service(\n		&quot;/statics/styles.css&quot;,\n		get_service(ServeFile::new(&quot;./statics/styles.css&quot;)),\n	)\n	.with_state(app_state)\n	.fallback(get(get_404_not_found));\n\nlet listener = tokio::net::TcpListener::bind(endpoint).await.unwrap();\naxum::serve(listener, app).await.unwrap();\n</code></pre>\n<h2>✅ Conclusion</h2>\n<p>In conclusion, the migration process is quite straightforward and not difficult. With this web framework migration, I can trim my dependencies count from <code>292</code> to <code>218</code>.</p>\n<p><img src="https://raw.githubusercontent.com/husni-zuhdi/husni-blog-resources/main/003-g-migrate-my-portfolio-website-from-actix-web-to-axum/../attachments/003_husni_portfolio_actix_web_dep_count.png" alt="Dependencies with actix-web" /></p>\n<p><img src="https://raw.githubusercontent.com/husni-zuhdi/husni-blog-resources/main/003-g-migrate-my-portfolio-website-from-actix-web-to-axum/../attachments/003_husni_portfolio_axum_dep_count.png" alt="Dependencies with axum" /></p>\n<p>The build duration also decreased by <code>40s</code> from <code>2m 28s</code> to <code>1m 49s</code>.</p>\n<p><img src="https://raw.githubusercontent.com/husni-zuhdi/husni-blog-resources/main/003-g-migrate-my-portfolio-website-from-actix-web-to-axum/../attachments/003_husni_portfolio_actix_web_build_dur.png" alt="Build time actix-web" /></p>\n<p><img src="https://raw.githubusercontent.com/husni-zuhdi/husni-blog-resources/main/003-g-migrate-my-portfolio-website-from-actix-web-to-axum/../attachments/003_husni_portfolio_axum_build_dur.png" alt="Build time axum" /></p>\n<p>Aside from all the upsides I write above, the only bug I encountered after migrating to <code>axum</code> is there is a blink appearing during a transition from one page to another. It''s just a minor bug and I''ll be working on that later.</p>\n<p><img src="https://raw.githubusercontent.com/husni-zuhdi/husni-blog-resources/main/003-g-migrate-my-portfolio-website-from-actix-web-to-axum/../attachments/003_husni_portfolio_blink.gif" alt="Blink bug" /></p>\n<p>I''ll share my migration story from the application performance side later. Please wait for the next blog. Thanks for reading.</p>\n','\n',char(10)));
INSERT INTO blogs VALUES('004-g','Effect of web framework migration on my portfolio website performance','Github','https://api.github.com/repos/husni-zuhdi/husni-blog-resources/contents/004-effect-of-web-framework-migration-on-my-portfolio-website-performance/README.md',replace('<h1>Effect of web framework migration on my portfolio website performance</h1>\n<h2>⛰ Background</h2>\n<p>In my previous blog about portfolio website migration (<a href="https://github.com/husni-zuhdi/husni-blog-resources/tree/main/003-migrate-my-portfolio-website-from-actix-web-to-axum">github</a> or <a href="https://husni-zuhdi.com/blogs/003-g">portfolio</a>) I shared my web framework migration process. It was a smooth process to migrate from <code>actix-web</code> to the <code>axum</code> web framework. I also describe the reasons for the migration which are:</p>\n<ul>\n<li>Trimming <code>husni-portfolio</code> dependencies</li>\n<li>Increase website performance</li>\n</ul>\n<p>We already trimmed dependencies from <code>292</code> to <code>218</code>. Now let''s see the pros and cons related to performance. The first step to check is benchmarks. I followed <a href="https://web-frameworks-benchmark.netlify.app/result?asc=0&amp;l=rust">Web Frameworks Benchmark</a> and <a href="https://github.com/programatik29/rust-web-benchmarks/blob/master/result/hello-world.md">Rust Web Benchmark</a> as references.</p>\n<p>From the first website, I can find facts that:</p>\n<ul>\n<li><code>actix</code> wins on throughput than <code>axum</code> or <a href="https://github.com/seanmonstar/warp">warp</a> (another tokio-based web framework). On Request per Second 512 (<em>I don''t know what 512 stands for</em>) <code>actix</code> got <code>620.690rps</code>, <code>warp</code> got <code>585.641rps</code>, and <code>axum</code> got <code>537.279rps</code>.</li>\n<li>On p99 latency, <code>warp</code> leads on <code>4.83ms</code> followed by <code>axum</code> on <code>5.31ms</code> then for <code>actix</code> it''s <code>8.58ms</code>. The p90 gives the same result.</li>\n</ul>\n<p>From the second website, I found this nice comparison:</p>\n<ul>\n<li><code>actix-web</code> has high throughput but high max latency. <img src="https://raw.githubusercontent.com/husni-zuhdi/husni-blog-resources/main/004-g-effect-of-web-framework-migration-on-my-portfolio-website-performance/../attachments/004_actix_web_performance.png" alt="actix-web performance" /></li>\n<li><code>axum</code> is more moderate RPS, has quite a low max latency (based on my gut feeling), and has the lowest max memory usage. <img src="https://raw.githubusercontent.com/husni-zuhdi/husni-blog-resources/main/004-g-effect-of-web-framework-migration-on-my-portfolio-website-performance/../attachments/004_axum_performance.png" alt="axum performance" /></li>\n<li><code>warp</code> has the lowest max latency among these three but also the lowest throughput. <img src="https://raw.githubusercontent.com/husni-zuhdi/husni-blog-resources/main/004-g-effect-of-web-framework-migration-on-my-portfolio-website-performance/../attachments/004_warp_performance.png" alt="warp performance" /></li>\n</ul>\n<p>I want my portfolio website to be fast and lightweight. I don''t expect to have a huge request coming to my website except for ThePrimagen to read my blogs or someone DDOS my website. So moderate throughput and low latency are good to have. I also don''t want my website to be memory-hungry since I want to press my infra cost as much as possible. Low memory usage is nice to have. Last, I choose <code>axum</code> instead of <code>warp</code> because <code>axum</code> has a more mature ecosystem with twice the Github Stargazer on their repo.</p>\n<h2>🏃‍♂️ Measure the performance</h2>\n<p>To measure my portfolio website performance, I use <a href="https://www.postman.com/">postman</a> and <a href="https://cloud.google.com/run">cloud run</a> metrics dashboard. <em>Postman is an API platform for building and using APIs. Postman simplifies each step of the API lifecycle and streamlines collaboration so you can create better APIs—faster</em>. It''s a <code>curl</code> wrapper with testing, automation, and collaboration features. While <em>Cloud Run is a managed compute platform that lets you run containers directly on top of Google''s scalable infrastructure</em>. I chose to use Cloud Run because it''s the cheapest option for running a container application. If I need to scale my applications, I might migrate all of my containers to Kubernetes.</p>\n<p>I created a <a href="https://github.com/husni-zuhdi/husni-portfolio/blob/main/husni_portfolio.postman_collection.json">postman collection</a>  to execute <code>Run</code> that will call my portfolio website endpoints:</p>\n<ul>\n<li><code>/</code> Home</li>\n<li><code>/blogs</code> Blogs</li>\n<li><code>/blogs/001</code> Blog 001</li>\n<li><code>/version</code> Version</li>\n<li><code>/not-found</code> Not Found</li>\n</ul>\n<pre><code class="language-json">{\n	&quot;info&quot;: {\n		&quot;_postman_id&quot;: &quot;70376775-4806-4ed9-8f10-a15a1ed903c0&quot;,\n		&quot;name&quot;: &quot;husni_portfolio&quot;,\n		&quot;description&quot;: &quot;Repo: [https://github.com/husni-zuhdi/husni-portfolio](https://github.com/husni-zuhdi/husni-portfolio)&quot;,\n		&quot;schema&quot;: &quot;https://schema.getpostman.com/json/collection/v2.1.0/collection.json&quot;,\n		&quot;_exporter_id&quot;: &quot;19888478&quot;\n	},\n	&quot;item&quot;: [\n		{\n			&quot;name&quot;: &quot;Home&quot;,\n			&quot;request&quot;: {\n				&quot;method&quot;: &quot;GET&quot;,\n				&quot;header&quot;: [],\n				&quot;url&quot;: {\n					&quot;raw&quot;: &quot;https://{{URL}}&quot;,\n					&quot;protocol&quot;: &quot;https&quot;,\n					&quot;host&quot;: [\n						&quot;{{URL}}&quot;\n					]\n				}\n			},\n			&quot;response&quot;: []\n		},\n		{\n			&quot;name&quot;: &quot;Blogs&quot;,\n			&quot;request&quot;: {\n				&quot;method&quot;: &quot;GET&quot;,\n				&quot;header&quot;: [],\n				&quot;url&quot;: {\n					&quot;raw&quot;: &quot;https://{{URL}}/blogs&quot;,\n					&quot;protocol&quot;: &quot;https&quot;,\n					&quot;host&quot;: [\n						&quot;{{URL}}&quot;\n					],\n					&quot;path&quot;: [\n						&quot;blogs&quot;\n					]\n				}\n			},\n			&quot;response&quot;: []\n		},\n		{\n			&quot;name&quot;: &quot;Blog 001&quot;,\n			&quot;request&quot;: {\n				&quot;method&quot;: &quot;GET&quot;,\n				&quot;header&quot;: [],\n				&quot;url&quot;: {\n					&quot;raw&quot;: &quot;https://{{URL}}/blogs/{{BLOG_ID}}&quot;,\n					&quot;protocol&quot;: &quot;https&quot;,\n					&quot;host&quot;: [\n						&quot;{{URL}}&quot;\n					],\n					&quot;path&quot;: [\n						&quot;blogs&quot;,\n						&quot;{{BLOG_ID}}&quot;\n					]\n				}\n			},\n			&quot;response&quot;: []\n		},\n		{\n			&quot;name&quot;: &quot;Version&quot;,\n			&quot;request&quot;: {\n				&quot;method&quot;: &quot;GET&quot;,\n				&quot;header&quot;: [],\n				&quot;url&quot;: {\n					&quot;raw&quot;: &quot;https://{{URL}}/version&quot;,\n					&quot;protocol&quot;: &quot;https&quot;,\n					&quot;host&quot;: [\n						&quot;{{URL}}&quot;\n					],\n					&quot;path&quot;: [\n						&quot;version&quot;\n					]\n				}\n			},\n			&quot;response&quot;: []\n		},\n		{\n			&quot;name&quot;: &quot;404 Not Found&quot;,\n			&quot;request&quot;: {\n				&quot;method&quot;: &quot;GET&quot;,\n				&quot;header&quot;: [],\n				&quot;url&quot;: {\n					&quot;raw&quot;: &quot;https://{{URL}}/not-found&quot;,\n					&quot;protocol&quot;: &quot;https&quot;,\n					&quot;host&quot;: [\n						&quot;{{URL}}&quot;\n					],\n					&quot;path&quot;: [\n						&quot;not-found&quot;\n					]\n				}\n			},\n			&quot;response&quot;: []\n		}\n	],\n	&quot;event&quot;: [\n		{\n			&quot;listen&quot;: &quot;prerequest&quot;,\n			&quot;script&quot;: {\n				&quot;type&quot;: &quot;text/javascript&quot;,\n				&quot;packages&quot;: {},\n				&quot;exec&quot;: [\n					&quot;&quot;\n				]\n			}\n		},\n		{\n			&quot;listen&quot;: &quot;test&quot;,\n			&quot;script&quot;: {\n				&quot;type&quot;: &quot;text/javascript&quot;,\n				&quot;packages&quot;: {},\n				&quot;exec&quot;: [\n					&quot;&quot;\n				]\n			}\n		}\n	],\n	&quot;variable&quot;: [\n		{\n			&quot;key&quot;: &quot;URL&quot;,\n			&quot;value&quot;: &quot;localhost:8080&quot;,\n			&quot;type&quot;: &quot;string&quot;\n		},\n		{\n			&quot;key&quot;: &quot;BLOG_ID&quot;,\n			&quot;value&quot;: &quot;001&quot;,\n			&quot;type&quot;: &quot;string&quot;\n		}\n	]\n}\n</code></pre>\n<p>To execute the <code>Run</code> you need to update the postman collection variables <code>URL</code> and <code>BLOG_ID</code>. The postman collection <code>Run</code> was executed in my workstation and iterated over 1000 endpoint calls and returned <code>duration</code> to complete a Run and <code>average response time</code> from all endpoints in a Run.</p>\n<p>During the postman <code>Run</code> execution I watched the Cloud Run metrics dashboard from the GCP console. The metrics that I watched are:</p>\n<ul>\n<li><code>Request count</code> to measure throughput in requests per second (rps).</li>\n<li><code>Request latencies</code> to measure average website latencies in 99% percentile (p99), 95% percentile (p95), and 50% percentile (p50).</li>\n<li><code>p99 Container CPU utilization</code> to measure the 99% percentile of container CPU utilization.</li>\n<li><code>p99 Container Memory utilization</code> to measure the 99% percentile of container CPU utilization.</li>\n<li><code>Max Send bytes</code> to measure the maximum bytes send through the internet.</li>\n</ul>\n<p>In the Cloud Run configuration, I set the CPU Limit and Request to <code>0.1m</code> and <code>0.08m</code>. For Memory Limit and Request I set both values to <code>128Mi</code>. Those values are the lowest resource configuration allowed for a Cloud Run service. You can read it further in the Cloud Run documentation about <a href="https://cloud.google.com/run/docs/configuring/services/cpu#cpu-memory">CPU Limit</a> and <a href="https://cloud.google.com/run/docs/configuring/services/memory-limits#memory-minimum">Minimum Memory Limit</a>\nWe are going to compare test results between the pre-migration version to the post-migration version.</p>\n<h2>⏱ Result</h2>\n<p>Here is the <em>first</em> test result (P for postman, CR for cloud run)</p>\n<table>\n<thead>\n<tr>\n<th>Metric</th>\n<th>Control(actix-web)</th>\n<th>Test #1(axum)</th>\n<th>Difference</th>\n<th>My facial expression</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>(P) Duration</td>\n<td>8m 39s</td>\n<td>4m 27s</td>\n<td>-48.5%</td>\n<td>😁</td>\n</tr>\n<tr>\n<td>(P) Average Response Time</td>\n<td>93ms</td>\n<td>43s</td>\n<td>-53.7%</td>\n<td>😁</td>\n</tr>\n<tr>\n<td>(CR) Request Count</td>\n<td>11.32rps</td>\n<td>18.67rps</td>\n<td>+64.9%</td>\n<td>🤩</td>\n</tr>\n<tr>\n<td>(CR) Request Latency p99</td>\n<td>141.46ms</td>\n<td>252.92ms</td>\n<td>+78.7%</td>\n<td>😧 wtf</td>\n</tr>\n<tr>\n<td>(CR) Request Latency p90</td>\n<td>55.35ms</td>\n<td>242.7ms</td>\n<td>+338.4%</td>\n<td>😭 WTF?</td>\n</tr>\n<tr>\n<td>(CR) Request Latency p50</td>\n<td>5.38ms</td>\n<td>64.22ms</td>\n<td>+1093.6%</td>\n<td>😱 WTF??!!!</td>\n</tr>\n<tr>\n<td>(CR) CPU Utilization p99</td>\n<td>12.99%</td>\n<td>32.98%</td>\n<td>+153.9%</td>\n<td>😢 what happened to latency?</td>\n</tr>\n<tr>\n<td>(CR) CPU Memory p99</td>\n<td>10.99%</td>\n<td>19.98%</td>\n<td>+81.8%</td>\n<td>😢 what happened to latency?</td>\n</tr>\n<tr>\n<td>(CR) Max Send Bytes</td>\n<td>124.24k/s</td>\n<td>209.85k/s</td>\n<td>+68%</td>\n<td>😢 what happened to latency?</td>\n</tr>\n</tbody>\n</table>\n<p>Based on the <code>My facial expression</code> column you can imagine my feeling on the test result above. I didn''t expect to have such high latencies after migrating to <code>axum</code> and here we are. This latency anomaly kept me puzzled for quite some time. Then after checking all config and data, I choose to do the second test. Here is the result</p>\n<table>\n<thead>\n<tr>\n<th>Metric</th>\n<th>Control(actix-web)</th>\n<th>Test #2(axum)</th>\n<th>Difference</th>\n<th>My facial expression</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>(P) Duration</td>\n<td>8m 39s</td>\n<td>5m 04s</td>\n<td>-41.4%</td>\n<td>🙏 please be good</td>\n</tr>\n<tr>\n<td>(P) Average Response Time</td>\n<td>93ms</td>\n<td>50s</td>\n<td>-46.2%</td>\n<td>🙏 please be good</td>\n</tr>\n<tr>\n<td>(CR) Request Count</td>\n<td>11.32rps</td>\n<td>18.1rps</td>\n<td>+59.9%</td>\n<td>🙏 please be good</td>\n</tr>\n<tr>\n<td>(CR) Request Latency p99</td>\n<td>141.46ms</td>\n<td>9.97ms</td>\n<td>-93.0%</td>\n<td>🤩</td>\n</tr>\n<tr>\n<td>(CR) Request Latency p90</td>\n<td>55.35ms</td>\n<td>9.57ms</td>\n<td>-82.7%</td>\n<td>🤩</td>\n</tr>\n<tr>\n<td>(CR) Request Latency p50</td>\n<td>5.38ms</td>\n<td>5.04ms</td>\n<td>-6.3%</td>\n<td>🤩</td>\n</tr>\n<tr>\n<td>(CR) CPU Utilization p99</td>\n<td>12.99%</td>\n<td>21.99%</td>\n<td>+69.3%</td>\n<td>🤩</td>\n</tr>\n<tr>\n<td>(CR) CPU Memory p99</td>\n<td>10.99%</td>\n<td>10.99%</td>\n<td>-</td>\n<td>🤩</td>\n</tr>\n<tr>\n<td>(CR) Max Send Bytes</td>\n<td>124.24k/s</td>\n<td>197.38k/s</td>\n<td>+58.9%</td>\n<td>🤩</td>\n</tr>\n</tbody>\n</table>\n<p>Now I know that the issue is not on the application side. It''s either on infrastructure or my testing methodology. So I traced back my cloud run <a href="https://terragrunt.gruntwork.io/">terragrunt</a> manifest in my private repo to the actual applied infrastructure configuration in Google Cloud Console. But I can''t find anything suspicious. Then I checked my Cloud Run metrics dashboard again because the <code>[P] Average Response Time</code> value with any latencies values doesn''t match. There must be something wrong with the metrics dashboard, I presume. Then I found this.</p>\n<p><img src="https://raw.githubusercontent.com/husni-zuhdi/husni-blog-resources/main/004-g-effect-of-web-framework-migration-on-my-portfolio-website-performance/../attachments/004_husni_portfolio_test_culprit.png" alt="The culprit" /></p>\n<p>Turns out, during the high latencies event there is no active container in Cloud Run. I suspect that the high latencies were attributed to the container startup time. These high latencies also happened outside the performance test time range as you can see in the Request count metric panel above. The test started between <code>20:08-20:09</code> but the high latency event happened a minute before the test. We can learn two things at least from this finding</p>\n<p><em>&quot;Never test once. At minimum test twice.</em></p>\n<p><em>Double-check the result data.&quot;</em></p>\n<p>Husni Zuhdi</p>\n<h2>✅ Conclusion</h2>\n<p>I learned so many things during this migration and performance testing. As a Site Reliability Engineer, I never have a first hand to test my application from scratch. The migration effort was also worth doing as the performance results are good. Improvement of <code>Request Count</code> 59.9% increase and  <code>p99 Request Latency</code> 93.0% decrease are all I need. The next thing I want to improve is the infrastructure side. I''ll post a new blog on how I optimize my portfolio website Cloud Run service cost to near $0. Please wait for the next blog. Thanks for reading.</p>\n','\n',char(10)));
INSERT INTO blogs VALUES('005-g','Optimize cloud run configuration for my portfolio website','Github','https://api.github.com/repos/husni-zuhdi/husni-blog-resources/contents/005-optimize-cloud-run-configuration-for-my-portfolio-website/README.md',replace('<h1>Optimize Cloud Run configuration for my portfolio website</h1>\n<h2>😕 What''s wrong?</h2>\n<p>After migrating my portfolio website and being satisfied with the performance of my previous blog (<a href="https://github.com/husni-zuhdi/husni-blog-resources/tree/main/004-effect-of-web-framework-migration-on-my-portfolio-website-performance">github</a> or <a href="https://husni-zuhdi.com/blogs/004-g">portfolio</a>), I started searching for other things I could optimize from the infrastructure side. This time my goal is to <strong>reduce the infrastructure cost as much as possible</strong>. For control data, we''ll use the cloud run cost in the past 10 days before the web framework migration (11 August 2024 - 21 August 2024) with resource configuration CPU Limit, CPU Request, and Memory Limit+Request  set to <code>0.1m</code>, <code>0.08m</code>, and <code>128Mi</code>. I mentioned it in my previous blog, but that resource configuration is the lowest Cloud Run allowed.</p>\n<p><img src="https://raw.githubusercontent.com/husni-zuhdi/husni-blog-resources/main/005-g-optimize-cloud-run-configuration-for-my-portfolio-website/../attachments/005_husni_portfolio_cloud_run_pre_migration_cost.png" alt="Cloud Run Cost in 10 days pre-migration" /></p>\n<p>Based on the image above, running a container for 10 days with the lowest resource configuration cost me $1. In a year it''ll cost me around $36 per container or IDR 565.000 (USD 1 = IDR 15.000). With the additional cost of domain renewal that cost me IDR 180.000 per year the total cost to run my portfolio website is IDR 745.000/year or <code>IDR 62.083/month</code>. That''s about the same cost for a Basic tier Netflix subscription!</p>\n<p><img src="https://raw.githubusercontent.com/husni-zuhdi/husni-blog-resources/main/005-g-optimize-cloud-run-configuration-for-my-portfolio-website/../attachments/005_netflix_sub_pricing_indonesia.png" alt="Netflix Subscription Price in Indonesia" /></p>\n<p>So in the spirit of saving infrastructure costs for better usage (not Netflix of course), let''s start our journey to optimize the Cloud Run cost.</p>\n<h2>🔧 Optimization</h2>\n<p>Let''s start with the details of the Cloud Run cost data before the migration. Drill down into the detail, we can see the top cost contributor of Cloud Run cost come from:</p>\n<ul>\n<li>Idle Min-Instance CPU Allocation Time (tier 2) that costs $0.5 per 10 days.</li>\n<li>Idle Min-Instance Memory Allocation Time (tier 2) that costs $0.44 per 10 days.</li>\n</ul>\n<p><img src="https://raw.githubusercontent.com/husni-zuhdi/husni-blog-resources/main/005-g-optimize-cloud-run-configuration-for-my-portfolio-website/../attachments/005_husni_portfolio_cloud_run_pre_migration_cost_sku.png" alt="Cloud Run Cost by SKU" /></p>\n<p>Looks like with the default Cloud Run configuration, I set the Minimum Idle Container active and keep my Cloud Run billing running. Upon further checking of <a href="https://github.com/GoogleCloudPlatform/terraform-google-cloud-run/blob/v0.9.1/modules/secure-cloud-run-core/variables.tf#L307-L311">Cloud Run terragrunt module</a> that I use, the default Minimum Idle Container was set to 1. Let''s change it to 0 and run the Postman performance test.</p>\n<pre><code class="language-hcl">	// pre-migration\n	max_scale_instances = 2\n	min_scale_instances = 1\n	\n	// post-migration\n	max_scale_instances = 2\n	min_scale_instances = 0\n</code></pre>\n<p>I ran the postman collection <code>Run</code> in my workstation and iterated over 1000 endpoint calls. We''ll use the <code>average response time</code> from all endpoints from this Run.</p>\n<p>From the Cloud Run metrics dashboard, I watched these metrics:</p>\n<ul>\n<li><code>Request count</code> to measure throughput in requests per second (rps).</li>\n<li><code>Request latencies</code> to measure average website latencies in 99% percentile (p99), 95% percentile (p95), and 50% percentile (p50).</li>\n<li><code>Max Billable Container Instance Time</code> to measure the maximum billable time in seconds of running/active/idle container(s).</li>\n<li><code>Container Startup Latency</code> to measure the time needed to start the container.</li>\n</ul>\n<h2>🚄 Result</h2>\n<p>Here is the test result (P for postman, CR for cloud run)</p>\n<table>\n<thead>\n<tr>\n<th>Metric</th>\n<th>Control</th>\n<th>Test Performance</th>\n<th>Difference</th>\n<th>Test Infra</th>\n<th>Difference</th>\n<th>Difference between test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>(P) Average Response Time</td>\n<td>93ms</td>\n<td>50ms</td>\n<td>-46.2%</td>\n<td>41ms</td>\n<td>-55.9%</td>\n<td>-9.7%</td>\n</tr>\n<tr>\n<td>(CR) Request Count</td>\n<td>11.32rps</td>\n<td>18.1rps</td>\n<td>+59.9%</td>\n<td>19.62rps</td>\n<td>+73.3%</td>\n<td>+13.4%</td>\n</tr>\n<tr>\n<td>(CR) Request Latency p99</td>\n<td>141.46ms</td>\n<td>9.97ms</td>\n<td>-93.0%</td>\n<td>9.9ms</td>\n<td>-93.0%</td>\n<td>0%</td>\n</tr>\n<tr>\n<td>(CR) Request Latency p90</td>\n<td>55.35ms</td>\n<td>9.57ms</td>\n<td>-82.7%</td>\n<td>9.5ms</td>\n<td>-82.8%</td>\n<td>-0.1%</td>\n</tr>\n<tr>\n<td>(CR) Request Latency p50</td>\n<td>5.38ms</td>\n<td>5.04ms</td>\n<td>-6.3%</td>\n<td>5ms</td>\n<td>-7.1%</td>\n<td>-0.8%</td>\n</tr>\n<tr>\n<td>(CR) Max Billable Container Instance Time</td>\n<td>1.001s/s</td>\n<td>1.001s/s</td>\n<td>0%</td>\n<td>0.9s/s</td>\n<td>-10.1%</td>\n<td>-10.1%</td>\n</tr>\n<tr>\n<td>(CR) Container Startup Latency</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n<td>1.2901s</td>\n<td>+1.2901s</td>\n<td>+1.2901s</td>\n</tr>\n</tbody>\n</table>\n<p>Above I added the <code>Test Performance</code> column and its <code>Difference</code> from the previous blog (<a href="">github</a> or <a href="">portfolio</a>) so we can compare it with the pre-migration portfolio website and post-migration portfolio website before Cloud Run optimization. From the postman <code>Average Response Time</code> and Cloud Run <code>Request Count</code> we get the better value after the optimization. There are not much difference on the latencies side but since the idle container was removed, it added latency called <code>Container Startup Latency</code> that took 1.2901s. This latency only shows up in the first request during the postman <code>Run</code> as we can see in the image below.</p>\n<p><img src="https://raw.githubusercontent.com/husni-zuhdi/husni-blog-resources/main/005-g-optimize-cloud-run-configuration-for-my-portfolio-website/../attachments/005_husni_portfolio_cr_opt.png" alt="Postman result" /></p>\n<p>Now let''s talk about the <code>Max Billable Container Instance Time</code> metric. From the optimization, we can reduce this metric by <code>10.1%</code> or theoretically reduce the Cloud Run monthly cost from $3 to $2.7 if the container always runs at peak. But that is not the daily case, instead most of the time the container will be downscaled to 0 as there is not much traffic to my portfolio website. So we can compare the trend of the <code>Max Billable Container Instance Time</code> metric pre-migration and post-optimization. In pre-migration, at least one container is always idle to serve the incoming traffic. However, in the post-optimization configuration, I don''t have any idle containers hence my cost when there is no traffic is $0. Since I''m willing to sacrifice availability for cost reduction, this is a big win!</p>\n<p><img src="https://raw.githubusercontent.com/husni-zuhdi/husni-blog-resources/main/005-g-optimize-cloud-run-configuration-for-my-portfolio-website/../attachments/005_husni_portfolio_billable_container_instance_comp.png" alt="Billable Container Instance Time Comparison" /></p>\n<p>Looking back to the Cloud Run cost dashboard a week after optimization, I don''t see any Cloud Run cost any more in the dashboard. With this cost optimization effort, I can reduce my Cloud Run <em>potential</em> cost from $36 per container or IDR 565.000 to $0 or $IDR. With a big &quot;IF&quot; of course because I might change my mind in the future.</p>\n<p><img src="https://raw.githubusercontent.com/husni-zuhdi/husni-blog-resources/main/005-g-optimize-cloud-run-configuration-for-my-portfolio-website/../attachments/005_husni_portfolio_post_opt_cost_dashboard.png" alt="Cloud Run Cost in 10 days post-optimization" /></p>\n<h2>✅ Conclusion</h2>\n<p>Thank you so much for reading my last blog on portfolio web framework migration. I have so much fun learning new things and writing all the blogs. I''m not sure what the next thing I''ll write about is. I have a plan to explore several features like SSO, OAuth, Feature Flag, and Observability with OpenTelemetry. Please wait for the next blog and as always thank you. Cheers!</p>\n','\n',char(10)));
COMMIT;
